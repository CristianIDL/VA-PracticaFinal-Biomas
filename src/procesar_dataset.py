'''
procesar_dataset.py: Procesa todo el dataset y extrae features para entrenamiento.
'''

import cv2
import numpy as np
import pickle
import time
from pathlib import Path
from src.segmentar_kmeans import cargar_imagen, segmentar_kmeans
from src.feature_extraction import extraer_features, obtener_nombres_features, calcular_dimensiones_features
from src.prints import crear_headline


def procesar_dataset(k=5, max_dimension=600):
    """
    Procesa todas las imÃ¡genes del dataset y extrae features.
    
    Args:
        k: NÃºmero de clusters para K-means
        max_dimension: DimensiÃ³n mÃ¡xima de imagen para procesamiento
    
    Returns:
        Diccionario con features, labels y metadata
    """
    
    print(crear_headline(f"PROCESAMIENTO DEL DATASET (K={k})"))
    
    # ConfiguraciÃ³n
    dataset_path = Path('data/raw')
    biomas = ['playa', 'montana', 'pradera', 'no_identificado']
    
    # Contenedores para datos
    todas_features = []
    todas_labels = []
    todos_filenames = []
    errores = []
    
    tiempo_inicio_total = time.time()
    total_imagenes = 0
    
    # Calcular total de imÃ¡genes
    for bioma in biomas:
        bioma_path = dataset_path / bioma
        if bioma_path.exists():
            total_imagenes += len(list(bioma_path.glob('*.jpg'))) + \
                            len(list(bioma_path.glob('*.png'))) + \
                            len(list(bioma_path.glob('*.jpeg')))
    
    print(f"\nðŸ“Š Total de imÃ¡genes a procesar: {total_imagenes}")
    print(f"ðŸ”¢ Clusters K-means: {k}")
    print(f"ðŸ“ Features por imagen: {calcular_dimensiones_features(k)}")
    print(f"{'='*60}\n")
    
    contador_global = 0
    
    # Procesar cada bioma
    for bioma in biomas:
        bioma_path = dataset_path / bioma
        
        if not bioma_path.exists():
            print(f"âš ï¸  Saltando {bioma}/ (no existe)\n")
            continue
        
        print(f"ðŸ“ Procesando: {bioma.upper()}")
        print(f"{'-'*60}")
        
        # Obtener todas las imÃ¡genes
        extensiones = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        archivos = []
        for ext in extensiones:
            archivos.extend(bioma_path.glob(ext))
        
        archivos = sorted(archivos)  # Ordenar para consistencia
        
        if len(archivos) == 0:
            print(f"âš ï¸  No hay imÃ¡genes en {bioma}/\n")
            continue
        
        # Procesar cada imagen
        for idx, archivo in enumerate(archivos, 1):
            contador_global += 1
            
            try:
                # Cargar imagen
                imagen_rgb = cargar_imagen(str(archivo), max_dimension=max_dimension)
                
                # Convertir a HSV para K-means
                imagen_hsv = cv2.cvtColor(imagen_rgb, cv2.COLOR_RGB2HSV)
                
                # Aplicar K-means (sin prints)
                altura, ancho, _ = imagen_hsv.shape
                pixeles = imagen_hsv.reshape((-1, 3))
                pixeles = np.float32(pixeles)
                
                criterio = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)
                _, etiquetas, centros = cv2.kmeans(
                    pixeles, k, None, criterio, 3, cv2.KMEANS_RANDOM_CENTERS
                )
                
                centros_hsv = np.uint8(centros)
                etiquetas = etiquetas.reshape((altura, ancho))
                
                # Extraer features
                features = extraer_features(imagen_rgb, etiquetas, centros_hsv, k)
                
                # Guardar datos
                todas_features.append(features)
                todas_labels.append(bioma)
                todos_filenames.append(str(archivo.name))
                
                # Progreso
                porcentaje = (contador_global / total_imagenes) * 100
                print(f"[{contador_global}/{total_imagenes}] ({porcentaje:5.1f}%) "
                      f"{archivo.name:30s} â†’ âœ“ Features extraÃ­das")
                
            except Exception as e:
                error_msg = f"{archivo.name}: {str(e)}"
                errores.append(error_msg)
                print(f"[{contador_global}/{total_imagenes}] "
                      f"{archivo.name:30s} â†’ âŒ ERROR: {str(e)}")
        
        print()  # Espacio entre biomas
    
    # Convertir a arrays numpy
    print(f"\n{'='*60}")
    print("ðŸ“¦ Consolidando datos...")
    
    X = np.array(todas_features, dtype=np.float32)
    y = np.array(todas_labels)
    filenames = np.array(todos_filenames)
    
    tiempo_total = time.time() - tiempo_inicio_total
    
    print(f"âœ“ Procesamiento completado en {tiempo_total:.2f} segundos")
    print(f"\nðŸ“Š EstadÃ­sticas:")
    print(f"  â€¢ ImÃ¡genes procesadas: {len(X)}")
    print(f"  â€¢ Errores: {len(errores)}")
    print(f"  â€¢ Shape de features: {X.shape}")
    print(f"  â€¢ Clases Ãºnicas: {np.unique(y)}")
    
    # DistribuciÃ³n por clase
    print(f"\nðŸ“ˆ DistribuciÃ³n por clase:")
    for bioma in biomas:
        count = np.sum(y == bioma)
        porcentaje = (count / len(y)) * 100 if len(y) > 0 else 0
        print(f"  â€¢ {bioma:20s}: {count:3d} ({porcentaje:5.1f}%)")
    
    # Mostrar errores si los hay
    if errores:
        print(f"\nâš ï¸  ERRORES ENCONTRADOS ({len(errores)}):")
        for error in errores[:10]:  # Mostrar mÃ¡ximo 10
            print(f"  â€¢ {error}")
        if len(errores) > 10:
            print(f"  ... y {len(errores) - 10} errores mÃ¡s")
    
    # Crear diccionario de datos
    datos = {
        'features': X,
        'labels': y,
        'filenames': filenames,
        'feature_names': obtener_nombres_features(k),
        'k': k,
        'n_samples': len(X),
        'n_features': X.shape[1],
        'biomas': biomas,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    # Guardar datos
    output_path = Path('data/processed')
    output_path.mkdir(parents=True, exist_ok=True)
    
    output_file = output_path / 'features.pkl'
    
    print(f"\nðŸ’¾ Guardando datos en: {output_file}")
    
    with open(output_file, 'wb') as f:
        pickle.dump(datos, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    tamano_mb = output_file.stat().st_size / (1024 * 1024)
    print(f"âœ“ Archivo guardado ({tamano_mb:.2f} MB)")
    
    print(f"\n{'='*60}")
    print("âœ“ PROCESAMIENTO COMPLETADO EXITOSAMENTE")
    print(f"{'='*60}")
    print(f"\nPrÃ³ximo paso: python train.py")
    
    return datos


def cargar_features():
    """Carga el archivo de features procesado."""
    features_path = Path('data/processed/features.pkl')
    
    if not features_path.exists():
        raise FileNotFoundError(
            f"No se encontrÃ³ {features_path}. "
            "Ejecuta primero: python procesar_dataset.py"
        )
    
    with open(features_path, 'rb') as f:
        datos = pickle.load(f)
    
    return datos


if __name__ == "__main__":
    # Procesar dataset con K=5
    datos = procesar_dataset(k=5, max_dimension=600)
    
    # Verificar que se puede cargar
    print("\nðŸ§ª Verificando que se puede cargar el archivo...")
    datos_cargados = cargar_features()
    print(f"âœ“ Archivo cargado correctamente")
    print(f"  Shape de features: {datos_cargados['features'].shape}")
    print(f"  Timestamp: {datos_cargados['timestamp']}")